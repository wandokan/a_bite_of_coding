# A_Bite_of_Coding
细嚼慢咽学DL

做这个项目的理念有二：

   1.在深度学习领域，为理论知识学习和实际完成项目搭桥。笔者个人而言在从理论知识到代码实现的过程中经过了比较艰苦的摸索过程，从而希望从个人角度出发为减小理论和实践的隔阂而做一点微小的贡献。
   
   2.温故知新，教学相长。在以“教”的视角，细嚼慢咽代码实现的过程中，注意到那些容易被忽略的细节，同时巩固个人的理解。
   
具体而言，本项目由浅入深，选取一些难度适宜、相关资料完整的深度学习小项目，进行精细化的代码注释，以帮助初学者对深度学习的模型和框架有更深的理解，助力从“懂了”到“会了”的过程。

鉴于笔者水平有限，不足之处在所难免，恳请各位专家学者批评指正，以求实现新的提升。

### Project 1：基于LSTM的神经翻译模型。

[ABC-NMT_with_RNNs](https://github.com/wandokan/ABC-NMT_with_RNNs)

选自斯坦福CS224N，2019 winter版本的Assignment 4。基于Pytorch，使用LSTM和注意力机制，构建了Spanish-English的翻译模型。

参考资料
   1. [PyTorch](https://pytorch.org/)
   2. [Stanford CS224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

### Project 2：Transformer（PyTorch）。

[ABC-Transformer](https://github.com/wandokan/ABC-Transformer)

选自Harvard NLP。对Transformer的经典复现。

参考资料
   1. [PyTorch](https://pytorch.org/)
   2. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
   3. [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

